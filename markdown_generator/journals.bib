@article{pintor2023imagenet,
  title={ImageNet-Patch: A dataset for benchmarking machine learning robustness against adversarial patches},
  author={Pintor, Maura and Angioni, Daniele and Sotgiu, Angelo and Demetrio, Luca and Demontis, Ambra and Biggio, Battista and Roli, Fabio},
  journal={Pattern Recognition},
  volume={134},
  pages={109064},
  year={2023},
  publisher={Elsevier},
    abstract = {Adversarial patches are optimized contiguous pixel blocks in an input image that cause a machine-learning model to misclassify it. However, their optimization is computationally demanding, and requires careful hyperparameter tuning, potentially leading to suboptimal robustness evaluations. To overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machine- learning models against adversarial patches. It consists of a set of patches, optimized to generalize across different models, and readily applicable to ImageNet data after preprocessing them with affine transformations. This process enables an approximate yet faster robustness evaluation, leveraging the transferability of adversarial perturbations. We showcase the usefulness of this dataset by testing the effectiveness of the computed patches against 127 models. We conclude by discussing how our dataset could be used as a benchmark for robustness, and how our methodology can be generalized to other domains. We open source our dataset and evaluation code at https://github.com/pralab/ImageNet-Patch.},
    url = {https://arxiv.org/abs/2203.04412},
    annote = {journals}
}


@article{PINTOR2022101095,
    title = {secml: Secure and explainable machine learning in Python},
    journal = {SoftwareX},
    volume = {18},
    pages = {101095},
    year = {2022},
    issn = {2352-7110},
    doi = {https://doi.org/10.1016/j.softx.2022.101095},
    url = {https://www.sciencedirect.com/science/article/pii/S2352711022000656},
    author = {Maura Pintor and Luca Demetrio and Angelo Sotgiu and Marco Melis and Ambra Demontis and Battista Biggio},
    keywords = {Machine learning, Security, Adversarial attacks, Explainability, Python3},
    abstract = {We present secml, an open-source Python library for secure and explainable machine learning. It implements the most popular attacks against machine learning, including test-time evasion attacks to generate adversarial examples against deep neural networks and training-time poisoning attacks against support vector machines and many other algorithms. These attacks enable evaluating the security of learning algorithms and the corresponding defenses under both white-box and black-box threat models. To this end, secml provides built-in functions to compute security evaluation curves, showing how quickly classification performance decreases against increasing adversarial perturbations of the input data. secml also includes explainability methods to help understand why adversarial attacks succeed against a given model, by visualizing the most influential features and training prototypes contributing to each decision. It is distributed under the Apache License 2.0 and hosted at https://github.com/pralab/secml.},
    annote = {journals}
}



@article{mirsky2022threat,
  title={The threat of offensive ai to organizations},
  author={Yisroel Mirsky and Ambra Demontis and Jaidip Kotak and Ram Shankar and Deng Gelei and Liu Yang and Xiangyu Zhang and Maura Pintor and Wenke Lee and Yuval Elovici and Battista Biggio},
  journal={Computers \& Security},
  pages={103006},
  year={2022},
  publisher={Elsevier},
  abstract={AI has provided us with the ability to automate tasks, extract information from vast amounts of data, and synthesize media that is nearly indistinguishable from the real thing. However, positive tools can also be used for negative purposes. In particular, cyber adversaries can use AI to enhance their attacks and expand their campaigns.
Although offensive AI has been discussed in the past, there is a need to analyze and understand the threat in the context of organizations. For example, how does an AI-capable adversary impact the cyber kill chain? Does AI benefit the attacker more than the defender? What are the most significant AI threats facing organizations today and what will be their impact on the future?
In this study, we explore the threat of offensive AI on organizations. First, we present the background and discuss how AI changes the adversaryâ€™s methods, strategies, goals, and overall attack model. Then, through a literature review, we identify 32 offensive AI capabilities which adversaries can use to enhance their attacks. Finally, through a panel survey spanning industry, government and academia, we rank the AI threats and provide insights on the adversaries.},
  annote = {journals}
}
