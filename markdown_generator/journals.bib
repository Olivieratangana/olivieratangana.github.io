@article{Pintor2022ImageNetPatchAD,
    title = {ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness against Adversarial Patches},
    author = {Maura Pintor and Daniele Angioni and Angelo Sotgiu and Luca Demetrio and Ambra Demontis and Battista Biggio and Fabio Roli},
    abstract = {Adversarial patches are optimized contiguous pixel blocks in an input image that cause a machine-learning model to misclassify it. However, their optimization is computationally demanding, and requires careful hyperparameter tuning, potentially leading to suboptimal robustness evaluations. To overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machine- learning models against adversarial patches. It consists of a set of patches, optimized to generalize across different models, and readily applicable to ImageNet data after preprocessing them with affine transformations. This process enables an approximate yet faster robustness evaluation, leveraging the transferability of adversarial perturbations. We showcase the usefulness of this dataset by testing the effectiveness of the computed patches against 127 models. We conclude by discussing how our dataset could be used as a benchmark for robustness, and how our methodology can be generalized to other domains. We open source our dataset and evaluation code at https://github.com/pralab/ImageNet-Patch.},
    journal = {Pattern Recognition},
    url = {https://arxiv.org/abs/2203.04412},
    year = {2022},
    volume = {abs/2203.04412},
    annote = {journals}
}

@article{PINTOR2022101095,
    title = {secml: Secure and explainable machine learning in Python},
    journal = {SoftwareX},
    volume = {18},
    pages = {101095},
    year = {2022},
    issn = {2352-7110},
    doi = {https://doi.org/10.1016/j.softx.2022.101095},
    url = {https://www.sciencedirect.com/science/article/pii/S2352711022000656},
    author = {Maura Pintor and Luca Demetrio and Angelo Sotgiu and Marco Melis and Ambra Demontis and Battista Biggio},
    keywords = {Machine learning, Security, Adversarial attacks, Explainability, Python3},
    abstract = {We present secml, an open-source Python library for secure and explainable machine learning. It implements the most popular attacks against machine learning, including test-time evasion attacks to generate adversarial examples against deep neural networks and training-time poisoning attacks against support vector machines and many other algorithms. These attacks enable evaluating the security of learning algorithms and the corresponding defenses under both white-box and black-box threat models. To this end, secml provides built-in functions to compute security evaluation curves, showing how quickly classification performance decreases against increasing adversarial perturbations of the input data. secml also includes explainability methods to help understand why adversarial attacks succeed against a given model, by visualizing the most influential features and training prototypes contributing to each decision. It is distributed under the Apache License 2.0 and hosted at https://github.com/pralab/secml.},
    annote = {journals}
}

